(venv) patelkev@instance-20240827-180431:~$ python train_mistral.py 
Using device: cuda
GPU Name: NVIDIA A100-SXM4-40GB
Total GPU Memory: 42.41 GB
Some weights of the model checkpoint at kevin36524/SVDfy_Mistral-7B_r32 were not used when initializing MistralForCausalLM: ['lm_head.0.weight', 'lm_head.1.weight', 'model.layers.0.mlp.down_proj.0.weight', 'model.layers.0.mlp.down_proj.1.weight', 'model.layers.0.mlp.gate_proj.0.weight', 'model.layers.0.mlp.gate_proj.1.weight', 'model.layers.0.mlp.up_proj.0.weight', 'model.layers.0.mlp.up_proj.1.weight', 'model.layers.0.self_attn.k_proj.0.weight', 'model.layers.0.self_attn.k_proj.1.weight', 'model.layers.0.self_attn.o_proj.0.weight', 'model.layers.0.self_attn.o_proj.1.weight', 'model.layers.0.self_attn.q_proj.0.weight', 'model.layers.0.self_attn.q_proj.1.weight', 'model.layers.0.self_attn.v_proj.0.weight', 'model.layers.0.self_attn.v_proj.1.weight', 'model.layers.1.mlp.down_proj.0.weight', 'model.layers.1.mlp.down_proj.1.weight', 'model.layers.1.mlp.gate_proj.0.weight', 'model.layers.1.mlp.gate_proj.1.weight', 'model.layers.1.mlp.up_proj.0.weight', 'model.layers.1.mlp.up_proj.1.weight', 'model.layers.1.self_attn.k_proj.0.weight', 'model.layers.1.self_attn.k_proj.1.weight', 'model.layers.1.self_attn.o_proj.0.weight', 'model.layers.1.self_attn.o_proj.1.weight', 'model.layers.1.self_attn.q_proj.0.weight', 'model.layers.1.self_attn.q_proj.1.weight', 'model.layers.1.self_attn.v_proj.0.weight', 'model.layers.1.self_attn.v_proj.1.weight', 'model.layers.10.mlp.down_proj.0.weight', 'model.layers.10.mlp.down_proj.1.weight', 'model.layers.10.mlp.gate_proj.0.weight', 'model.layers.10.mlp.gate_proj.1.weight', 'model.layers.10.mlp.up_proj.0.weight', 'model.layers.10.mlp.up_proj.1.weight', 'model.layers.10.self_attn.k_proj.0.weight', 'model.layers.10.self_attn.k_proj.1.weight', 'model.layers.10.self_attn.o_proj.0.weight', 'model.layers.10.self_attn.o_proj.1.weight', 'model.layers.10.self_attn.q_proj.0.weight', 'model.layers.10.self_attn.q_proj.1.weight', 'model.layers.10.self_attn.v_proj.0.weight', 'model.layers.10.self_attn.v_proj.1.weight', 'model.layers.11.mlp.down_proj.0.weight', 'model.layers.11.mlp.down_proj.1.weight', 'model.layers.11.mlp.gate_proj.0.weight', 'model.layers.11.mlp.gate_proj.1.weight', 'model.layers.11.mlp.up_proj.0.weight', 'model.layers.11.mlp.up_proj.1.weight', 'model.layers.11.self_attn.k_proj.0.weight', 'model.layers.11.self_attn.k_proj.1.weight', 'model.layers.11.self_attn.o_proj.0.weight', 'model.layers.11.self_attn.o_proj.1.weight', 'model.layers.11.self_attn.q_proj.0.weight', 'model.layers.11.self_attn.q_proj.1.weight', 'model.layers.11.self_attn.v_proj.0.weight', 'model.layers.11.self_attn.v_proj.1.weight', 'model.layers.12.mlp.down_proj.0.weight', 'model.layers.12.mlp.down_proj.1.weight', 'model.layers.12.mlp.gate_proj.0.weight', 'model.layers.12.mlp.gate_proj.1.weight', 'model.layers.12.mlp.up_proj.0.weight', 'model.layers.12.mlp.up_proj.1.weight', 'model.layers.12.self_attn.k_proj.0.weight', 'model.layers.12.self_attn.k_proj.1.weight', 'model.layers.12.self_attn.o_proj.0.weight', 'model.layers.12.self_attn.o_proj.1.weight', 'model.layers.12.self_attn.q_proj.0.weight', 'model.layers.12.self_attn.q_proj.1.weight', 'model.layers.12.self_attn.v_proj.0.weight', 'model.layers.12.self_attn.v_proj.1.weight', 'model.layers.13.mlp.down_proj.0.weight', 'model.layers.13.mlp.down_proj.1.weight', 'model.layers.13.mlp.gate_proj.0.weight', 'model.layers.13.mlp.gate_proj.1.weight', 'model.layers.13.mlp.up_proj.0.weight', 'model.layers.13.mlp.up_proj.1.weight', 'model.layers.13.self_attn.k_proj.0.weight', 'model.layers.13.self_attn.k_proj.1.weight', 'model.layers.13.self_attn.o_proj.0.weight', 'model.layers.13.self_attn.o_proj.1.weight', 'model.layers.13.self_attn.q_proj.0.weight', 'model.layers.13.self_attn.q_proj.1.weight', 'model.layers.13.self_attn.v_proj.0.weight', 'model.layers.13.self_attn.v_proj.1.weight', 'model.layers.14.mlp.down_proj.0.weight', 'model.layers.14.mlp.down_proj.1.weight', 'model.layers.14.mlp.gate_proj.0.weight', 'model.layers.14.mlp.gate_proj.1.weight', 'model.layers.14.mlp.up_proj.0.weight', 'model.layers.14.mlp.up_proj.1.weight', 'model.layers.14.self_attn.k_proj.0.weight', 'model.layers.14.self_attn.k_proj.1.weight', 'model.layers.14.self_attn.o_proj.0.weight', 'model.layers.14.self_attn.o_proj.1.weight', 'model.layers.14.self_attn.q_proj.0.weight', 'model.layers.14.self_attn.q_proj.1.weight', 'model.layers.14.self_attn.v_proj.0.weight', 'model.layers.14.self_attn.v_proj.1.weight', 'model.layers.15.mlp.down_proj.0.weight', 'model.layers.15.mlp.down_proj.1.weight', 'model.layers.15.mlp.gate_proj.0.weight', 'model.layers.15.mlp.gate_proj.1.weight', 'model.layers.15.mlp.up_proj.0.weight', 'model.layers.15.mlp.up_proj.1.weight', 'model.layers.15.self_attn.k_proj.0.weight', 'model.layers.15.self_attn.k_proj.1.weight', 'model.layers.15.self_attn.o_proj.0.weight', 'model.layers.15.self_attn.o_proj.1.weight', 'model.layers.15.self_attn.q_proj.0.weight', 'model.layers.15.self_attn.q_proj.1.weight', 'model.layers.15.self_attn.v_proj.0.weight', 'model.layers.15.self_attn.v_proj.1.weight', 'model.layers.16.mlp.down_proj.0.weight', 'model.layers.16.mlp.down_proj.1.weight', 'model.layers.16.mlp.gate_proj.0.weight', 'model.layers.16.mlp.gate_proj.1.weight', 'model.layers.16.mlp.up_proj.0.weight', 'model.layers.16.mlp.up_proj.1.weight', 'model.layers.16.self_attn.k_proj.0.weight', 'model.layers.16.self_attn.k_proj.1.weight', 'model.layers.16.self_attn.o_proj.0.weight', 'model.layers.16.self_attn.o_proj.1.weight', 'model.layers.16.self_attn.q_proj.0.weight', 'model.layers.16.self_attn.q_proj.1.weight', 'model.layers.16.self_attn.v_proj.0.weight', 'model.layers.16.self_attn.v_proj.1.weight', 'model.layers.17.mlp.down_proj.0.weight', 'model.layers.17.mlp.down_proj.1.weight', 'model.layers.17.mlp.gate_proj.0.weight', 'model.layers.17.mlp.gate_proj.1.weight', 'model.layers.17.mlp.up_proj.0.weight', 'model.layers.17.mlp.up_proj.1.weight', 'model.layers.17.self_attn.k_proj.0.weight', 'model.layers.17.self_attn.k_proj.1.weight', 'model.layers.17.self_attn.o_proj.0.weight', 'model.layers.17.self_attn.o_proj.1.weight', 'model.layers.17.self_attn.q_proj.0.weight', 'model.layers.17.self_attn.q_proj.1.weight', 'model.layers.17.self_attn.v_proj.0.weight', 'model.layers.17.self_attn.v_proj.1.weight', 'model.layers.18.mlp.down_proj.0.weight', 'model.layers.18.mlp.down_proj.1.weight', 'model.layers.18.mlp.gate_proj.0.weight', 'model.layers.18.mlp.gate_proj.1.weight', 'model.layers.18.mlp.up_proj.0.weight', 'model.layers.18.mlp.up_proj.1.weight', 'model.layers.18.self_attn.k_proj.0.weight', 'model.layers.18.self_attn.k_proj.1.weight', 'model.layers.18.self_attn.o_proj.0.weight', 'model.layers.18.self_attn.o_proj.1.weight', 'model.layers.18.self_attn.q_proj.0.weight', 'model.layers.18.self_attn.q_proj.1.weight', 'model.layers.18.self_attn.v_proj.0.weight', 'model.layers.18.self_attn.v_proj.1.weight', 'model.layers.19.mlp.down_proj.0.weight', 'model.layers.19.mlp.down_proj.1.weight', 'model.layers.19.mlp.gate_proj.0.weight', 'model.layers.19.mlp.gate_proj.1.weight', 'model.layers.19.mlp.up_proj.0.weight', 'model.layers.19.mlp.up_proj.1.weight', 'model.layers.19.self_attn.k_proj.0.weight', 'model.layers.19.self_attn.k_proj.1.weight', 'model.layers.19.self_attn.o_proj.0.weight', 'model.layers.19.self_attn.o_proj.1.weight', 'model.layers.19.self_attn.q_proj.0.weight', 'model.layers.19.self_attn.q_proj.1.weight', 'model.layers.19.self_attn.v_proj.0.weight', 'model.layers.19.self_attn.v_proj.1.weight', 'model.layers.2.mlp.down_proj.0.weight', 'model.layers.2.mlp.down_proj.1.weight', 'model.layers.2.mlp.gate_proj.0.weight', 'model.layers.2.mlp.gate_proj.1.weight', 'model.layers.2.mlp.up_proj.0.weight', 'model.layers.2.mlp.up_proj.1.weight', 'model.layers.2.self_attn.k_proj.0.weight', 'model.layers.2.self_attn.k_proj.1.weight', 'model.layers.2.self_attn.o_proj.0.weight', 'model.layers.2.self_attn.o_proj.1.weight', 'model.layers.2.self_attn.q_proj.0.weight', 'model.layers.2.self_attn.q_proj.1.weight', 'model.layers.2.self_attn.v_proj.0.weight', 'model.layers.2.self_attn.v_proj.1.weight', 'model.layers.20.mlp.down_proj.0.weight', 'model.layers.20.mlp.down_proj.1.weight', 'model.layers.20.mlp.gate_proj.0.weight', 'model.layers.20.mlp.gate_proj.1.weight', 'model.layers.20.mlp.up_proj.0.weight', 'model.layers.20.mlp.up_proj.1.weight', 'model.layers.20.self_attn.k_proj.0.weight', 'model.layers.20.self_attn.k_proj.1.weight', 'model.layers.20.self_attn.o_proj.0.weight', 'model.layers.20.self_attn.o_proj.1.weight', 'model.layers.20.self_attn.q_proj.0.weight', 'model.layers.20.self_attn.q_proj.1.weight', 'model.layers.20.self_attn.v_proj.0.weight', 'model.layers.20.self_attn.v_proj.1.weight', 'model.layers.21.mlp.down_proj.0.weight', 'model.layers.21.mlp.down_proj.1.weight', 'model.layers.21.mlp.gate_proj.0.weight', 'model.layers.21.mlp.gate_proj.1.weight', 'model.layers.21.mlp.up_proj.0.weight', 'model.layers.21.mlp.up_proj.1.weight', 'model.layers.21.self_attn.k_proj.0.weight', 'model.layers.21.self_attn.k_proj.1.weight', 'model.layers.21.self_attn.o_proj.0.weight', 'model.layers.21.self_attn.o_proj.1.weight', 'model.layers.21.self_attn.q_proj.0.weight', 'model.layers.21.self_attn.q_proj.1.weight', 'model.layers.21.self_attn.v_proj.0.weight', 'model.layers.21.self_attn.v_proj.1.weight', 'model.layers.22.mlp.down_proj.0.weight', 'model.layers.22.mlp.down_proj.1.weight', 'model.layers.22.mlp.gate_proj.0.weight', 'model.layers.22.mlp.gate_proj.1.weight', 'model.layers.22.mlp.up_proj.0.weight', 'model.layers.22.mlp.up_proj.1.weight', 'model.layers.22.self_attn.k_proj.0.weight', 'model.layers.22.self_attn.k_proj.1.weight', 'model.layers.22.self_attn.o_proj.0.weight', 'model.layers.22.self_attn.o_proj.1.weight', 'model.layers.22.self_attn.q_proj.0.weight', 'model.layers.22.self_attn.q_proj.1.weight', 'model.layers.22.self_attn.v_proj.0.weight', 'model.layers.22.self_attn.v_proj.1.weight', 'model.layers.23.mlp.down_proj.0.weight', 'model.layers.23.mlp.down_proj.1.weight', 'model.layers.23.mlp.gate_proj.0.weight', 'model.layers.23.mlp.gate_proj.1.weight', 'model.layers.23.mlp.up_proj.0.weight', 'model.layers.23.mlp.up_proj.1.weight', 'model.layers.23.self_attn.k_proj.0.weight', 'model.layers.23.self_attn.k_proj.1.weight', 'model.layers.23.self_attn.o_proj.0.weight', 'model.layers.23.self_attn.o_proj.1.weight', 'model.layers.23.self_attn.q_proj.0.weight', 'model.layers.23.self_attn.q_proj.1.weight', 'model.layers.23.self_attn.v_proj.0.weight', 'model.layers.23.self_attn.v_proj.1.weight', 'model.layers.24.mlp.down_proj.0.weight', 'model.layers.24.mlp.down_proj.1.weight', 'model.layers.24.mlp.gate_proj.0.weight', 'model.layers.24.mlp.gate_proj.1.weight', 'model.layers.24.mlp.up_proj.0.weight', 'model.layers.24.mlp.up_proj.1.weight', 'model.layers.24.self_attn.k_proj.0.weight', 'model.layers.24.self_attn.k_proj.1.weight', 'model.layers.24.self_attn.o_proj.0.weight', 'model.layers.24.self_attn.o_proj.1.weight', 'model.layers.24.self_attn.q_proj.0.weight', 'model.layers.24.self_attn.q_proj.1.weight', 'model.layers.24.self_attn.v_proj.0.weight', 'model.layers.24.self_attn.v_proj.1.weight', 'model.layers.25.mlp.down_proj.0.weight', 'model.layers.25.mlp.down_proj.1.weight', 'model.layers.25.mlp.gate_proj.0.weight', 'model.layers.25.mlp.gate_proj.1.weight', 'model.layers.25.mlp.up_proj.0.weight', 'model.layers.25.mlp.up_proj.1.weight', 'model.layers.25.self_attn.k_proj.0.weight', 'model.layers.25.self_attn.k_proj.1.weight', 'model.layers.25.self_attn.o_proj.0.weight', 'model.layers.25.self_attn.o_proj.1.weight', 'model.layers.25.self_attn.q_proj.0.weight', 'model.layers.25.self_attn.q_proj.1.weight', 'model.layers.25.self_attn.v_proj.0.weight', 'model.layers.25.self_attn.v_proj.1.weight', 'model.layers.26.mlp.down_proj.0.weight', 'model.layers.26.mlp.down_proj.1.weight', 'model.layers.26.mlp.gate_proj.0.weight', 'model.layers.26.mlp.gate_proj.1.weight', 'model.layers.26.mlp.up_proj.0.weight', 'model.layers.26.mlp.up_proj.1.weight', 'model.layers.26.self_attn.k_proj.0.weight', 'model.layers.26.self_attn.k_proj.1.weight', 'model.layers.26.self_attn.o_proj.0.weight', 'model.layers.26.self_attn.o_proj.1.weight', 'model.layers.26.self_attn.q_proj.0.weight', 'model.layers.26.self_attn.q_proj.1.weight', 'model.layers.26.self_attn.v_proj.0.weight', 'model.layers.26.self_attn.v_proj.1.weight', 'model.layers.27.mlp.down_proj.0.weight', 'model.layers.27.mlp.down_proj.1.weight', 'model.layers.27.mlp.gate_proj.0.weight', 'model.layers.27.mlp.gate_proj.1.weight', 'model.layers.27.mlp.up_proj.0.weight', 'model.layers.27.mlp.up_proj.1.weight', 'model.layers.27.self_attn.k_proj.0.weight', 'model.layers.27.self_attn.k_proj.1.weight', 'model.layers.27.self_attn.o_proj.0.weight', 'model.layers.27.self_attn.o_proj.1.weight', 'model.layers.27.self_attn.q_proj.0.weight', 'model.layers.27.self_attn.q_proj.1.weight', 'model.layers.27.self_attn.v_proj.0.weight', 'model.layers.27.self_attn.v_proj.1.weight', 'model.layers.28.mlp.down_proj.0.weight', 'model.layers.28.mlp.down_proj.1.weight', 'model.layers.28.mlp.gate_proj.0.weight', 'model.layers.28.mlp.gate_proj.1.weight', 'model.layers.28.mlp.up_proj.0.weight', 'model.layers.28.mlp.up_proj.1.weight', 'model.layers.28.self_attn.k_proj.0.weight', 'model.layers.28.self_attn.k_proj.1.weight', 'model.layers.28.self_attn.o_proj.0.weight', 'model.layers.28.self_attn.o_proj.1.weight', 'model.layers.28.self_attn.q_proj.0.weight', 'model.layers.28.self_attn.q_proj.1.weight', 'model.layers.28.self_attn.v_proj.0.weight', 'model.layers.28.self_attn.v_proj.1.weight', 'model.layers.29.mlp.down_proj.0.weight', 'model.layers.29.mlp.down_proj.1.weight', 'model.layers.29.mlp.gate_proj.0.weight', 'model.layers.29.mlp.gate_proj.1.weight', 'model.layers.29.mlp.up_proj.0.weight', 'model.layers.29.mlp.up_proj.1.weight', 'model.layers.29.self_attn.k_proj.0.weight', 'model.layers.29.self_attn.k_proj.1.weight', 'model.layers.29.self_attn.o_proj.0.weight', 'model.layers.29.self_attn.o_proj.1.weight', 'model.layers.29.self_attn.q_proj.0.weight', 'model.layers.29.self_attn.q_proj.1.weight', 'model.layers.29.self_attn.v_proj.0.weight', 'model.layers.29.self_attn.v_proj.1.weight', 'model.layers.3.mlp.down_proj.0.weight', 'model.layers.3.mlp.down_proj.1.weight', 'model.layers.3.mlp.gate_proj.0.weight', 'model.layers.3.mlp.gate_proj.1.weight', 'model.layers.3.mlp.up_proj.0.weight', 'model.layers.3.mlp.up_proj.1.weight', 'model.layers.3.self_attn.k_proj.0.weight', 'model.layers.3.self_attn.k_proj.1.weight', 'model.layers.3.self_attn.o_proj.0.weight', 'model.layers.3.self_attn.o_proj.1.weight', 'model.layers.3.self_attn.q_proj.0.weight', 'model.layers.3.self_attn.q_proj.1.weight', 'model.layers.3.self_attn.v_proj.0.weight', 'model.layers.3.self_attn.v_proj.1.weight', 'model.layers.30.mlp.down_proj.0.weight', 'model.layers.30.mlp.down_proj.1.weight', 'model.layers.30.mlp.gate_proj.0.weight', 'model.layers.30.mlp.gate_proj.1.weight', 'model.layers.30.mlp.up_proj.0.weight', 'model.layers.30.mlp.up_proj.1.weight', 'model.layers.30.self_attn.k_proj.0.weight', 'model.layers.30.self_attn.k_proj.1.weight', 'model.layers.30.self_attn.o_proj.0.weight', 'model.layers.30.self_attn.o_proj.1.weight', 'model.layers.30.self_attn.q_proj.0.weight', 'model.layers.30.self_attn.q_proj.1.weight', 'model.layers.30.self_attn.v_proj.0.weight', 'model.layers.30.self_attn.v_proj.1.weight', 'model.layers.31.mlp.down_proj.0.weight', 'model.layers.31.mlp.down_proj.1.weight', 'model.layers.31.mlp.gate_proj.0.weight', 'model.layers.31.mlp.gate_proj.1.weight', 'model.layers.31.mlp.up_proj.0.weight', 'model.layers.31.mlp.up_proj.1.weight', 'model.layers.31.self_attn.k_proj.0.weight', 'model.layers.31.self_attn.k_proj.1.weight', 'model.layers.31.self_attn.o_proj.0.weight', 'model.layers.31.self_attn.o_proj.1.weight', 'model.layers.31.self_attn.q_proj.0.weight', 'model.layers.31.self_attn.q_proj.1.weight', 'model.layers.31.self_attn.v_proj.0.weight', 'model.layers.31.self_attn.v_proj.1.weight', 'model.layers.4.mlp.down_proj.0.weight', 'model.layers.4.mlp.down_proj.1.weight', 'model.layers.4.mlp.gate_proj.0.weight', 'model.layers.4.mlp.gate_proj.1.weight', 'model.layers.4.mlp.up_proj.0.weight', 'model.layers.4.mlp.up_proj.1.weight', 'model.layers.4.self_attn.k_proj.0.weight', 'model.layers.4.self_attn.k_proj.1.weight', 'model.layers.4.self_attn.o_proj.0.weight', 'model.layers.4.self_attn.o_proj.1.weight', 'model.layers.4.self_attn.q_proj.0.weight', 'model.layers.4.self_attn.q_proj.1.weight', 'model.layers.4.self_attn.v_proj.0.weight', 'model.layers.4.self_attn.v_proj.1.weight', 'model.layers.5.mlp.down_proj.0.weight', 'model.layers.5.mlp.down_proj.1.weight', 'model.layers.5.mlp.gate_proj.0.weight', 'model.layers.5.mlp.gate_proj.1.weight', 'model.layers.5.mlp.up_proj.0.weight', 'model.layers.5.mlp.up_proj.1.weight', 'model.layers.5.self_attn.k_proj.0.weight', 'model.layers.5.self_attn.k_proj.1.weight', 'model.layers.5.self_attn.o_proj.0.weight', 'model.layers.5.self_attn.o_proj.1.weight', 'model.layers.5.self_attn.q_proj.0.weight', 'model.layers.5.self_attn.q_proj.1.weight', 'model.layers.5.self_attn.v_proj.0.weight', 'model.layers.5.self_attn.v_proj.1.weight', 'model.layers.6.mlp.down_proj.0.weight', 'model.layers.6.mlp.down_proj.1.weight', 'model.layers.6.mlp.gate_proj.0.weight', 'model.layers.6.mlp.gate_proj.1.weight', 'model.layers.6.mlp.up_proj.0.weight', 'model.layers.6.mlp.up_proj.1.weight', 'model.layers.6.self_attn.k_proj.0.weight', 'model.layers.6.self_attn.k_proj.1.weight', 'model.layers.6.self_attn.o_proj.0.weight', 'model.layers.6.self_attn.o_proj.1.weight', 'model.layers.6.self_attn.q_proj.0.weight', 'model.layers.6.self_attn.q_proj.1.weight', 'model.layers.6.self_attn.v_proj.0.weight', 'model.layers.6.self_attn.v_proj.1.weight', 'model.layers.7.mlp.down_proj.0.weight', 'model.layers.7.mlp.down_proj.1.weight', 'model.layers.7.mlp.gate_proj.0.weight', 'model.layers.7.mlp.gate_proj.1.weight', 'model.layers.7.mlp.up_proj.0.weight', 'model.layers.7.mlp.up_proj.1.weight', 'model.layers.7.self_attn.k_proj.0.weight', 'model.layers.7.self_attn.k_proj.1.weight', 'model.layers.7.self_attn.o_proj.0.weight', 'model.layers.7.self_attn.o_proj.1.weight', 'model.layers.7.self_attn.q_proj.0.weight', 'model.layers.7.self_attn.q_proj.1.weight', 'model.layers.7.self_attn.v_proj.0.weight', 'model.layers.7.self_attn.v_proj.1.weight', 'model.layers.8.mlp.down_proj.0.weight', 'model.layers.8.mlp.down_proj.1.weight', 'model.layers.8.mlp.gate_proj.0.weight', 'model.layers.8.mlp.gate_proj.1.weight', 'model.layers.8.mlp.up_proj.0.weight', 'model.layers.8.mlp.up_proj.1.weight', 'model.layers.8.self_attn.k_proj.0.weight', 'model.layers.8.self_attn.k_proj.1.weight', 'model.layers.8.self_attn.o_proj.0.weight', 'model.layers.8.self_attn.o_proj.1.weight', 'model.layers.8.self_attn.q_proj.0.weight', 'model.layers.8.self_attn.q_proj.1.weight', 'model.layers.8.self_attn.v_proj.0.weight', 'model.layers.8.self_attn.v_proj.1.weight', 'model.layers.9.mlp.down_proj.0.weight', 'model.layers.9.mlp.down_proj.1.weight', 'model.layers.9.mlp.gate_proj.0.weight', 'model.layers.9.mlp.gate_proj.1.weight', 'model.layers.9.mlp.up_proj.0.weight', 'model.layers.9.mlp.up_proj.1.weight', 'model.layers.9.self_attn.k_proj.0.weight', 'model.layers.9.self_attn.k_proj.1.weight', 'model.layers.9.self_attn.o_proj.0.weight', 'model.layers.9.self_attn.o_proj.1.weight', 'model.layers.9.self_attn.q_proj.0.weight', 'model.layers.9.self_attn.q_proj.1.weight', 'model.layers.9.self_attn.v_proj.0.weight', 'model.layers.9.self_attn.v_proj.1.weight']
- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MistralForCausalLM were not initialized from the model checkpoint at kevin36524/SVDfy_Mistral-7B_r32 and are newly initialized: ['lm_head.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'text': Value(dtype='string', id=None)}
Trainable parameters: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'lm_head.weight']
/home/patelkev/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/patelkev/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/patelkev/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/patelkev/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:407: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
  0%|                                                                            | 0/57 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/patelkev/train_mistral.py", line 76, in <module>
    trainer.train()
  File "/home/patelkev/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 450, in train
    output = super().train(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3318, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3363, in compute_loss
    outputs = model(**inputs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1033, in forward
    outputs = self.model(
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 808, in forward
    layer_outputs = decoder_layer(
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 549, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 443, in forward
    query_states = self.q_proj(hidden_states)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patelkev/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 
  0%|          | 0/57 [00:00<?, ?it/s] 
